{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ankus\\OneDrive\\Desktop\\JAN_2025\\RAG\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [\"ankush is a boy, aadhaar number is 12324242, he likes pan pizza\",\"ankush is a boy pan number is 232525 and needs to have tablet named dopamicnine\"]\n",
    "embeddings = model.encode(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limitations\n",
    "\n",
    "# Sentence transformers often work by encoding sentences into fixed-size vectors, which can lose some of the subtle context or longer dependencies between words. While they capture semantic meaning, they might struggle with understanding nuanced or long-range relationships that span multiple sentences or paragraphs.\n",
    "\n",
    "# Sentence transformers can be computationally expensive, especially when processing large datasets or performing inference on long texts. This is particularly an issue for transformer models with many layers, such as BERT-based models, which require significant memory and processing power.\n",
    "# Inability to Handle OOV (Out of Vocabulary) Words Well:\n",
    "\n",
    "# While models like BERT or RoBERTa are based on subword tokenization (e.g., Byte Pair Encoding), they still face challenges with handling rare or completely out-of-vocabulary words. The embedding of these words can sometimes be imprecise, potentially leading to less accurate results.\n",
    "# Biases:\n",
    "\n",
    "# Like many machine learning models, sentence transformers can inherit biases from the data they are trained on. This could lead to biased sentence embeddings when used for tasks such as sentiment analysis, information retrieval, or other NLP tasks.\n",
    "# Dependence on Pre-trained Data:\n",
    "\n",
    "# Sentence transformers rely on pre-trained models, which means their performance is heavily influenced by the quality of the training corpus. If the training data doesn't cover certain topics or domains well, the model's performance may degrade on tasks outside the domain it was trained on.\n",
    "# Difficulty in Handling Ambiguity:\n",
    "\n",
    "# Sentence transformers may struggle with sentences that are highly ambiguous or contain contradictory information. Since the model relies on vector representations of sentences, it might fail to distinguish subtle differences between sentences that require deep understanding or reasoning.\n",
    "# Limited Interpretability:\n",
    "\n",
    "# The embeddings generated by sentence transformers are high-dimensional vectors, which can be difficult to interpret directly. While there are methods to analyze them, they do not inherently provide insights into why a particular sentence was encoded in a certain way, making them less transparent.\n",
    "# Performance on Short Texts:\n",
    "\n",
    "# While sentence transformers work well for full sentences or longer texts, their performance on very short texts (e.g., single-word inputs or short phrases) can be suboptimal since the representation might lack enough information to form a meaningful embedding.\n",
    "# Lack of Fine-Tuning for Specific Tasks:\n",
    "\n",
    "# While general-purpose pre-trained models perform well in a variety of scenarios, they may not always perform optimally on highly specialized tasks without fine-tuning. Fine-tuning requires a domain-specific dataset, which may not always be available.\n",
    "# Difficulty with Out-of-Context Phrases:\n",
    "\n",
    "# Sentence transformers are typically trained to handle sentences in context, and they may have difficulty when a sentence or phrase is presented out of its original context. For example, using a sentence fragment in isolation may lead to less accurate embeddings because the model misses surrounding cues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-win_amd64.whl (7.5 kB)\n",
      "Collecting tensorflow-intel==2.18.0\n",
      "  Downloading tensorflow_intel-2.18.0-cp39-cp39-win_amd64.whl (390.0 MB)\n",
      "     -------------------------------------- 390.0/390.0 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "     ---------------------------------------- 26.4/26.4 MB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (58.1.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0\n",
      "  Downloading ml_dtypes-0.4.1-cp39-cp39-win_amd64.whl (126 kB)\n",
      "     -------------------------------------- 126.7/126.7 KB 1.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.9/71.9 KB 2.0 MB/s eta 0:00:00\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.12.1-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.0-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Collecting keras>=3.5.0\n",
      "  Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-5.29.2-cp39-cp39-win_amd64.whl (434 kB)\n",
      "     -------------------------------------- 434.6/434.6 KB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.69.0-cp39-cp39-win_amd64.whl (4.4 MB)\n",
      "     ---------------------------------------- 4.4/4.4 MB 4.5 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.7/133.7 KB 3.9 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "     ---------------------------------------- 72.5/72.5 KB 3.9 MB/s eta 0:00:00\n",
      "Collecting optree\n",
      "  Downloading optree-0.13.1-cp39-cp39-win_amd64.whl (277 kB)\n",
      "     -------------------------------------- 277.7/277.7 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "     -------------------------------------- 242.4/242.4 KB 1.5 MB/s eta 0:00:00\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.3/106.3 KB 2.1 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "     -------------------------------------- 224.5/224.5 KB 2.3 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.5/87.5 KB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\ankus\\onedrive\\desktop\\jan_2025\\rag\\.venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.21.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, h5py, grpcio, google-pasta, gast, absl-py, markdown-it-py, markdown, astunparse, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.12.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.69.0 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 protobuf-5.29.2 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ankus\\OneDrive\\Desktop\\JAN_2025\\RAG\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ankus\\OneDrive\\Desktop\\JAN_2025\\RAG\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ankus\\OneDrive\\Desktop\\JAN_2025\\RAG\\.venv\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ankus\\OneDrive\\Desktop\\JAN_2025\\RAG\\.venv\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Universal Sentence Encoder (USE)\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "# Load Sentence-BERT (SBERT) model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"The teacher enthusiastically explained the complex topic, ensuring that every student understood the concepts thoroughly.\"\n",
    "sentence2 = \"The instructor eagerly described the intricate subject, making sure that each learner grasped the ideas completely.\"\n",
    "#SBERT WON\n",
    "\n",
    "\n",
    "sentence1 = \"X loves Y\"\n",
    "sentence2 = \"Y is loved by X\"\n",
    "#SBERT WON\n",
    "\n",
    "sentence1 = \"the cat sat on the windowsill in the sunlight\"\n",
    "sentence2 = \"the cat rested on the windowsill under the sun\"\n",
    "#SBERT WON\n",
    "\n",
    "sentence1 = \"she is reading a book in the garden\"  \n",
    "sentence2 = \"she sits in the garden, reading the book\"\n",
    "#SBERT WON\n",
    "\n",
    "sentence1 = \"he went to the store to buy groceries\"\n",
    "sentence2 = \"he visited the shop to get some groceries\"\n",
    "\n",
    "#finding grocerries and groceries have affect on the store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity using Universal Sentence Encoder (USE): 0.795874\n",
      "Cosine Similarity using Sentence-BERT (SBERT): 0.895548\n",
      "\n",
      "Embedding size for Universal Sentence Encoder (USE): (512,)\n",
      "Embedding size for Sentence-BERT (SBERT): (1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings using USE\n",
    "use_embeddings1 = use_model([sentence1])[0].numpy()\n",
    "use_embeddings2 = use_model([sentence2])[0].numpy()\n",
    "\n",
    "\n",
    "# Get the embeddings using SBERT\n",
    "sbert_embeddings1 = sbert_model.encode([sentence1])\n",
    "sbert_embeddings2 = sbert_model.encode([sentence2])\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Cosine Similarity for USE\n",
    "use_similarity = cosine_similarity([use_embeddings1], [use_embeddings2])[0][0]\n",
    "\n",
    "# Calculate Cosine Similarity for SBERT\n",
    "sbert_similarity = cosine_similarity(sbert_embeddings1, sbert_embeddings2)[0][0]\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine Similarity using Universal Sentence Encoder (USE):\", use_similarity)\n",
    "print(\"Cosine Similarity using Sentence-BERT (SBERT):\", sbert_similarity)\n",
    "\n",
    "print(\"\\nEmbedding size for Universal Sentence Encoder (USE):\", use_embeddings1.shape)\n",
    "print(\"Embedding size for Sentence-BERT (SBERT):\", sbert_embeddings1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can observe sizes 512 for 384, although 384 size is less but captures sementic relations properly.\n",
    "# Question is does bigger embedding size capture context better ?\n",
    "\n",
    "# Yes, in general, a larger embedding size can help capture more contextual information and semantic nuances. However, the relationship between embedding size and contextual understanding is nuanced. \n",
    "\n",
    "# The benefits of a larger embedding size depend on the task. For simple tasks like text classification or sentiment analysis, the difference in performance between a 384-dimensional and 768-dimensional model may be small. However, for more complex tasks like semantic textual similarity or paraphrase detection, a larger embedding size might yield better results, since capturing subtle differences in meaning is more critical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
